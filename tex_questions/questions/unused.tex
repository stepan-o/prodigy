\begin{figure}[hbt!]
    \centering
    \includegraphics[width=1\linewidth,trim=1 1 1 1,clip]{img/adaline_dag.jpg}
    \caption{Adaline algorithm compares the true class labels with the linear activation function's continuous valued output to compute the model error and update the weights.
    In contrast, the perceptron compares the true class labels to the predicted class labels.}
    \label{fig:adaline_dag}
\end{figure}


\begin{equation}
    \begin{split}
        \frac{ \pa{J} } { \pa{w_j} } & =
        \frac{\partial} {\pa{w_j}} \frac{1} {2} \sum \limits_i \left( y^{(i)} - \phi \left( z^{(i)} \right) \right) ^2 = \\
        & = \frac{1} {2} \frac{\partial} {\pa{w_j}} \sum \limits_i \left( y^{(i)} - \phi \left( z^{(i)} \right) \right) ^2 = \\
        & = \frac{1} {2} \sum \limits_i 2 \left( y^{(i)} - \phi \left( z^{(i)} \right) \right) \frac{\partial} {\pa{w_j}} \left( y^{(i)} - \phi \left( z^{(i)} \right) \right) = \\
        & = \sum \limits_i \left( y^{(i)} - \phi \left( z^{(i)} \right) \right) \frac{\partial} {\pa{w_j}} \left( y^{(i)} - \sum \limits_i \left( w_j^{(i)} x_j^{(i)} \right) \right) = \\
        & = \sum \limits_i \left( y^{(i)} - \phi \left( z^{(i)} \right) \right) \left( - x_j^{(i)} \right) = \\
        & = -\sum \limits_i \left( y^{(i)} - \phi \left( z^{(i)} \right) \right) x_j^{(i)}
    \end{split}
\end{equation}

\begin{figure}[hbt!]
    \centering
    \begin{subfigure}[t]{.48\textwidth}
        \centering
        \includegraphics[width=\columnwidth,trim=4 4 4 4,clip]{img/ada_learn_rate_001.png}
        \caption{$\eta=0.01$}
        \label{fig:ada_learn_rate_001}
    \end{subfigure}
    ~ %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc.
    %(or a blank line to force the subfigure onto a new line)
    \begin{subfigure}[t]{.48\textwidth}
        \centering
        \includegraphics[width=\columnwidth,trim=4 4 4 4,clip]{img/ada_learn_rate_00001.png}
        \caption{$\eta=0.0001$}
        \label{fig:ada_learn_rate_00001}
    \end{subfigure}
    \caption{It often requires some experimentation to find a good learning rate $\eta$ for optimal convergence.
    Two different learning rates, $\eta=0.1$ (\ref{fig:ada_learn_rate_001}) and $\eta=0.0001$(\ref{fig:ada_learn_rate_00001}), were chosen and the cost functions versus the number of epochs were plotted to see how well the Adaline implementation learns from the training data.}
    \label{fig:ada_learn_rate}
\end{figure}

It is considered common practice to use discount factors $G(\tau) = \sum \limits_{t=0}^\infty \lambda^t r_t $ for both infinite- and finite-horizon (**episodic**) tasks to show preference for actions that are closer in time with respect to those that will be encountered in the distant future.
However, in the case of the current formulation of the question-feeding bot, all the rewards are considered to be equally important and bot's tasks are assumed to have a finite and short horizon;
thus, simple return formulation with no discount factors has been used.

Based on the conditions outlined above, the following reward structure can be used for the task list-building agent:
At any time step $t$, after the player answers a question, the reward received by the bot will be driven by the relative difficulty of the question and whether it was answered right or not.

|Question $Q_t$ | Answer $Ans_t$ | Learning $L_t$ | Frustration $F_t$ | Reward $R_t$ |
|---------------|----------------|----------------|-------------------|--------------|
|     Easy      |      Corr      |      + 0.5     |       + 0.5       |    + 1.0     |
|     Easy      |     Incorr     |        0.0     |       - 0.5       |    - 0.5     |
|    Average    |      Corr      |      + 1.5     |       + 0.5       |    + 2.0     |
|    Average    |     Incorr     |      + 0.5     |       - 0.5       |      0.0     |
|  Challenging  |      Corr      |      + 2.5     |       + 0.5       |    + 3.0     |
|  Challenging  |     Incorr     |      + 1.0     |       - 0.5       |    + 0.5     |
